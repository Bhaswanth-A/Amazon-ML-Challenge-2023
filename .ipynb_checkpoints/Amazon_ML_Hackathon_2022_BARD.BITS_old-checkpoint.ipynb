{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406f78f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Input, Dropout, Dense, concatenate, GRU, Embedding, Flatten, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4997a40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train_df = pd.read_csv('train.csv') #read train dataset\n",
    "test_df = pd.read_csv('test.csv') # read test dataset\n",
    "print(train_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d8fb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminate all the outliers in the PRODUCT_LENGTH column using IQR method\n",
    "outliers = []\n",
    "def detect_outliers_iqr(data):\n",
    "    median = data['PRODUCT_LENGTH'].median()\n",
    "\n",
    "    q1 = np.percentile(data['PRODUCT_LENGTH'], 25)\n",
    "    q3 = np.percentile(data['PRODUCT_LENGTH'], 75)\n",
    "    IQR = q3-q1\n",
    "    lwr_bound = q1-(1.5*IQR)\n",
    "    upr_bound = q3+(1.5*IQR)\n",
    "    count =0\n",
    "    for i in range(len(data['PRODUCT_LENGTH'])): \n",
    "        if (data.iat[i,-1]<lwr_bound or data.iat[i,-1]>upr_bound):\n",
    "            data.iat[i,-1] = median\n",
    "            count = count +1 \n",
    "    return count\n",
    "count = detect_outliers_iqr(train_df)\n",
    "print(\"Outliers from IQR method: \", count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93af46ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing data.\n",
    "def fill_missing_values(df):\n",
    "    df.TITLE.fillna(value=\"Missing\", inplace=True)\n",
    "    df.BULLET_POINTS.fillna(value=\"Missing\", inplace=True)\n",
    "    df.DESCRIPTION.fillna(value=\"Missing\", inplace=True)\n",
    "    return df\n",
    "\n",
    "train_df = fill_missing_values(train_df)\n",
    "test_df = fill_missing_values(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98dab47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38399d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation_symbols = []\n",
    "for symbol in punctuation:\n",
    "    punctuation_symbols.append((symbol, ''))\n",
    "    \n",
    "punctuation_symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbdd1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e292357",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc52ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "# Create a function to remove punctuations\n",
    "def remove_punctuation(sentence: str) -> str:\n",
    "    return sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# Create a function to remove stop words\n",
    "def remove_stop_words(x):\n",
    "    x = ' '.join([i for i in x.lower().split(' ') if i not in stop])\n",
    "    return x\n",
    "\n",
    "# Create a function to lowercase the words\n",
    "def to_lower(x):\n",
    "    return x.lower()\n",
    "\n",
    "# Create a function to remove emojis\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags \n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116f96ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale target variable to log.\n",
    "train_df['target'] = np.log1p(train_df['PRODUCT_LENGTH'])\n",
    "\n",
    "Y_train = train_df.target.values.reshape(-1,1)\n",
    "\n",
    "# Calculate number of train/dev/test examples.\n",
    "n_trains = train_df.shape[0]\n",
    "print(\"Training on\", n_trains, \"examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a841f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.concat([train_df, test_df]) #combine the train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa856cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "full_df['DESCRIPTION'] = full_df['DESCRIPTION'].apply(porter.stem)\n",
    "full_df['DESCRIPTION'] = full_df['DESCRIPTION'].apply(remove_emoji)\n",
    "full_df['DESCRIPTION'] = full_df['DESCRIPTION'].apply(remove_punctuation)\n",
    "full_df['DESCRIPTION'] = full_df['DESCRIPTION'].apply(remove_stop_words)\n",
    "full_df['DESCRIPTION'] = full_df['DESCRIPTION'].apply(to_lower)\n",
    "\n",
    "full_df['TITLE'] = full_df['TITLE'].apply(remove_punctuation)\n",
    "full_df['TITLE'] = full_df['TITLE'].apply(remove_emoji)\n",
    "full_df['TITLE'] = full_df['TITLE'].apply(remove_stop_words)\n",
    "full_df['TITLE'] = full_df['TITLE'].apply(to_lower)\n",
    "\n",
    "full_df['BULLET_POINTS'] = full_df['BULLET_POINTS'].apply(porter.stem)\n",
    "full_df['BULLET_POINTS'] = full_df['BULLET_POINTS'].apply(remove_emoji)\n",
    "full_df['BULLET_POINTS'] = full_df['BULLET_POINTS'].apply(remove_punctuation)\n",
    "full_df['BULLET_POINTS'] = full_df['BULLET_POINTS'].apply(remove_stop_words)\n",
    "full_df['BULLET_POINTS'] = full_df['BULLET_POINTS'].apply(to_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d5f715",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.drop('PRODUCT_ID', axis=1, inplace=True) #drop PRODUCT_ID column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232bca5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Convert data type to string\n",
    "full_df['PRODUCT_TYPE_ID'] = full_df['PRODUCT_TYPE_ID'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79324069",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Vectorization of the data in the columns\n",
    "print(\"Vectorizing data...\")\n",
    "default_preprocessor = CountVectorizer().build_preprocessor()\n",
    "def build_preprocessor(field):\n",
    "    field_idx = list(full_df.columns).index(field)\n",
    "    return lambda x: default_preprocessor(x[field_idx])\n",
    "\n",
    "vectorizer = FeatureUnion([\n",
    "    ('TITLE', CountVectorizer(\n",
    "        ngram_range=(1, 3),\n",
    "        max_features=None,\n",
    "        preprocessor=build_preprocessor('TITLE'))),\n",
    "    ('PRODUCT_TYPE_ID', CountVectorizer(\n",
    "        token_pattern='\\d+',\n",
    "        preprocessor=build_preprocessor('PRODUCT_TYPE_ID'))),\n",
    "    ('DESCRIPTION', TfidfVectorizer(\n",
    "        ngram_range=(1, 3),\n",
    "        max_features=None,\n",
    "        preprocessor=build_preprocessor('DESCRIPTION'))),\n",
    "    ('BULLET_POINTS', TfidfVectorizer(\n",
    "        ngram_range=(1, 3),\n",
    "        max_features=None,\n",
    "        preprocessor=build_preprocessor('BULLET_POINTS'))),\n",
    "])\n",
    "\n",
    "X = vectorizer.fit_transform(full_df.values) #sparse matrix containing all the vectorized columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae4be5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the combined sparse matrix into train and test\n",
    "X_train = X[:n_trains] \n",
    "X_test = X[n_trains:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f0799b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape, X_train.shape, X_test.shape, Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37337350",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Train a Ridge Regression model on the obtained X_train and Y_train \n",
    "print(\"Fitting Ridge model on training examples...\")\n",
    "ridge_model = Ridge(\n",
    "    solver='auto', fit_intercept=True, alpha=15,\n",
    "    max_iter=60, normalize=False, tol=0.05,\n",
    ")\n",
    "ridge_model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5361e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ridge_preds = ridge_model.predict(X_test) #predict X_test using the trained Ridge model\n",
    "ridge_preds = np.expm1(ridge_preds) # perform anti-log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55424b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_preds # is the final 2x2 array containing the PRODUCT_LENGTHs of the test dataset\n",
    "test_df['PRODUCT_LENGTH'] = ridge_preds\n",
    "test_df['PRODUCT_LENGTH'] = test_df['PRODUCT_LENGTH'].apply(pd.Series).astype(int) #convert the array into int\n",
    "test_df.drop(['TITLE', 'BULLET_POINTS', 'DESCRIPTION', 'PRODUCT_TYPE_ID'], axis=1, inplace=True) \n",
    "test_df.to_csv('final_predicted_lengths.csv')\n",
    "test_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
